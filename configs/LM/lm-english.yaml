# lm related
lm: transformer
lm_conf:
  pos_enc:
  embed_unit: 128
  att_unit: 512
  head: 8
  unit: 2048
  layer: 16
  dropout_rate: 0.0

# espnet model related
model_conf:
  ignore_id: -1

# other details
init:
token_type: char
bpemodel:
token_list: [
  "<blank>",
  "<unk>",
  "'",
  "0",
  "1",
  "2",
  "3",
  "4",
  "5",
  "6",
  "7",
  "8",
  "9",
  "<space>",
  "A",
  "B",
  "C",
  "D",
  "E",
  "F",
  "G",
  "H",
  "I",
  "J",
  "K",
  "L",
  "M",
  "N",
  "O",
  "P",
  "Q",
  "R",
  "S",
  "T",
  "U",
  "V",
  "W",
  "X",
  "Y",
  "Z",
  "<sos/eos>",
  ]

# training related
epochs: 50
accum_grad: 1
average_epochs: 10
dtype: "float32"
device: "cuda:0"

